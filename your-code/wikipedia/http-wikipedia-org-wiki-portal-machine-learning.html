<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Portal:Machine learning - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"Portal","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":100,"wgPageName":"Portal:Machine_learning","wgTitle":"Machine learning","wgCurRevisionId":887475509,"wgRevisionId":887475509,"wgArticleId":44942806,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Portals with short description","Single-page portals","All portals","Portals with titles not starting with a proper noun","Machine learning","Computing portals"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Portal:Machine_learning","wgRelevantArticleId":44942806,"wgRequestId":"XIuUPgpAMFkAADcsUtcAAABF","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q58630879","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true,"oresWikiId":"enwiki","oresBaseUrl":"http://ores.discovery.wmnet:8081/","oresApiVersion":3});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","ext.categoryTree.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.math.scripts","mediawiki.page.gallery.slideshow","ext.scribunto.logs","ext.categoryTree","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.categoryTree.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.33.0-wmf.21" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<link href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/Portal:Machine_learning" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/load.php?lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-100 ns-subject mw-editable page-Portal_Machine_learning rootpage-Portal_Machine_learning skin-vector action-view"> <div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Portal:Machine learning</h1> <div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div> <div id="contentSub"></div>
<div id="jump-to-nav"></div> <a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Wikipedia's portal for exploring content related to Machine learning</div>
<div class="portal-maintenance-status" style="display:none;">
<table class="plainlinks ombox ombox-notice" role="presentation"><tbody><tr><td class="mbox-image"><a class="image" href="/wiki/File:Darkgreen_flag_waving.svg"><img alt="Darkgreen flag waving.svg" data-file-height="268" data-file-width="249" decoding="async" height="32" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/30px-Darkgreen_flag_waving.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/45px-Darkgreen_flag_waving.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Darkgreen_flag_waving.svg/60px-Darkgreen_flag_waving.svg.png 2x" width="30"/></a></td><td class="mbox-text"><span style="font-size:108%;"><b>Portal maintenance status:</b></span> <small>(September 2018)</small>
<ul><li>This portal has a <b>single page layout</b>. Any <a href="/wiki/Special:PrefixIndex/Portal:Machine_learning/" title="Special:PrefixIndex/Portal:Machine learning/">subpages</a> are likely no longer needed.</li></ul>
<span style="font-size:90%;">Please <a class="mw-redirect" href="/wiki/Wikipedia:CAREFUL" title="Wikipedia:CAREFUL">take care</a> when editing, especially if using <a class="mw-redirect" href="/wiki/Wikipedia:ASSISTED" title="Wikipedia:ASSISTED">automated editing software</a>. Learn how to <a href="/wiki/Template:Portal_maintenance_status#How_to_update_the_maintenance_information_for_a_portal" title="Template:Portal maintenance status">update the maintenance information here</a>.</span></td></tr></tbody></table></div>
<div class="hlist noprint" id="portals-browsebar" style="text-align: center;">
<dl><dt><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">Portal topics</a></dt>
<dd><a href="/wiki/Portal:Contents/Portals#Human_activities" title="Portal:Contents/Portals">Activities</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Culture_and_the_arts" title="Portal:Contents/Portals">Culture</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Geography_and_places" title="Portal:Contents/Portals">Geography</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Health_and_fitness" title="Portal:Contents/Portals">Health</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#History_and_events" title="Portal:Contents/Portals">History</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Mathematics_and_logic" title="Portal:Contents/Portals">Mathematics</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Natural_and_physical_sciences" title="Portal:Contents/Portals">Nature</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#People_and_self" title="Portal:Contents/Portals">People</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Philosophy_and_thinking" title="Portal:Contents/Portals">Philosophy</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Religion_and_belief_systems" title="Portal:Contents/Portals">Religion</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Society_and_social_sciences" title="Portal:Contents/Portals">Society</a></dd>
<dd><a href="/wiki/Portal:Contents/Portals#Technology_and_applied_sciences" title="Portal:Contents/Portals">Technology</a></dd>
<dd><a href="/wiki/Special:RandomInCategory/All_portals" title="Special:RandomInCategory/All portals">Random portal</a></dd></dl>
</div>
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Introduction">Introduction</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<p><b><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></b> (ML) is the <a href="/wiki/Branches_of_science" title="Branches of science">scientific study</a> of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> and <a href="/wiki/Statistical_model" title="Statistical model">statistical models</a> that <a class="mw-redirect" href="/wiki/Computer_systems" title="Computer systems">computer systems</a> use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. Machine learning algorithms build a mathematical model of sample data, known as "<a class="mw-redirect" href="/wiki/Training_data" title="Training data">training data</a>", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as <a href="/wiki/Email_filtering" title="Email filtering">email filtering</a>, detection of network intruders, and <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to <a href="/wiki/Computational_statistics" title="Computational statistics">computational statistics</a>, which focuses on making predictions using computers. The study of <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">mathematical optimization</a> delivers methods, theory and application domains to the field of machine learning. <a href="/wiki/Data_mining" title="Data mining">Data mining</a> is a field of study within machine learning, and focuses on <a href="/wiki/Exploratory_data_analysis" title="Exploratory data analysis">exploratory data analysis</a> through <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>. In its application across business problems, machine learning is also referred to as <a href="/wiki/Predictive_analytics" title="Predictive analytics">predictive analytics</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b><a href="/wiki/Machine_learning" title="Machine learning">Read more...</a></b></div><div style="clear:both;"></div></div>
<div style="text-align:center; margin:0.25em auto 0.75em"><span class="noprint plainlinks purgelink"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><span title="Purge this page"><b>Refresh with new selections below</b> (purge)</span></a></span></div>
<style data-mw-deduplicate="TemplateStyles:r886281085">.mw-parser-output .flex-columns-container{clear:both;width:100%;display:flex;flex-wrap:wrap}.mw-parser-output .flex-columns-container>.flex-columns-column{float:left;width:50%;min-width:360px;padding:0 0.5em;box-sizing:border-box;flex:1;display:flex;flex-direction:column}@media screen and (max-width:393px){.mw-parser-output .flex-columns-container>.flex-columns-column{min-width:0}}.mw-parser-output .flex-columns-container>.flex-columns-column:first-child{padding-left:0}.mw-parser-output .flex-columns-container>.flex-columns-column:last-child{padding-right:0}@media screen and (max-width:720px){.mw-parser-output .flex-columns-container>.flex-columns-column{padding:0;width:100%}.mw-parser-output .flex-columns-container{display:block}}.mw-parser-output .flex-columns-container>.flex-columns-column>div{flex:1 0 auto}.mw-parser-output .flex-columns-container>.flex-columns-column>div.flex-columns-noflex{flex:0}</style><div class="flex-columns-container"><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_general_articles">Selected general articles</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046835">.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(2){display:none}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div:nth-child(1){padding-bottom:0}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+2){display:none}.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox,.mw-parser-output div.excerptSlideshow-container .gallery .gallerybox div{width:100%!important;max-width:100%}.mw-parser-output div.excerptSlideshow-container>ul.gallery.mw-gallery-slideshow>li:not(.gallerycarousel)>div>div:nth-child(1){display:none}</style><div class="excerptSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">The <b><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">Conference and Workshop on Neural Information Processing Systems</a></b> (abbreviated as <b>NeurIPS</b> and formerly <b>NIPS</b>)  is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Computational_neuroscience" title="Computational neuroscience">computational neuroscience</a> <a href="/wiki/Academic_conference" title="Academic conference">conference</a> held every December.  The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts. <b><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Vapnik–Chervonenkis theory</a></b> (also known as <b>VC theory</b>) was developed during 1960–1990 by <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a> and <a href="/wiki/Alexey_Chervonenkis" title="Alexey Chervonenkis">Alexey Chervonenkis</a>. The theory is a form of <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a>, which attempts to explain the learning process from a statistical point of view.<br/><br/>VC theory is related to <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a>  and to <a class="mw-redirect" href="/wiki/Empirical_processes" title="Empirical processes">empirical processes</a>.  <a href="/wiki/Richard_M._Dudley" title="Richard M. Dudley">Richard M. Dudley</a> and <a href="/wiki/Vladimir_Vapnik" title="Vladimir Vapnik">Vladimir Vapnik</a>, among others, have applied VC-theory to <a class="mw-redirect" href="/wiki/Empirical_processes" title="Empirical processes">empirical processes</a>. <b><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov Model</a></b> (<b>HMM</b>) is a <a href="/wiki/Statistical_model" title="Statistical model">statistical</a> <a href="/wiki/Markov_model" title="Markov model">Markov model</a> in which the system being modeled is assumed to be a <a class="mw-redirect" href="/wiki/Markov_process" title="Markov process">Markov process</a> with unobserved (i.e. <i>hidden</i>) states.<br/><br/>The hidden Markov model can be represented as the simplest <a href="/wiki/Dynamic_Bayesian_network" title="Dynamic Bayesian network">dynamic Bayesian network</a>. The mathematics behind the HMM were developed by <a href="/wiki/Leonard_E._Baum" title="Leonard E. Baum">L. E. Baum</a> and coworkers.<br/> HMM is closely related to earlier work on the optimal nonlinear <a href="/wiki/Filtering_problem_(stochastic_processes)" title="Filtering problem (stochastic processes)">filtering problem</a> by <a class="mw-redirect" href="/wiki/Ruslan_L._Stratonovich" title="Ruslan L. Stratonovich">Ruslan L. Stratonovich</a>, who was the first to describe the <a href="/wiki/Forward%E2%80%93backward_algorithm" title="Forward–backward algorithm">forward-backward procedure</a>. <b><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, a <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a></b> (<b>CNN</b>, or <b>ConvNet</b>) is a class of <a class="mw-redirect" href="/wiki/Deep_neural_network" title="Deep neural network">deep neural networks</a>, most commonly applied to analyzing visual imagery.<br/><br/>CNNs use a variation of <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> designed to require minimal <a href="/wiki/Data_pre-processing" title="Data pre-processing">preprocessing</a>. They are also known as <b>shift invariant</b> or <b>space invariant artificial neural networks</b> (<b>SIANN</b>), based on their shared-weights architecture and <a class="mw-redirect" href="/wiki/Translation_invariance" title="Translation invariance">translation invariance</a> characteristics. <b><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference</a></b> (<b>TD</b>) <b>learning</b> refers to a class of <a href="/wiki/Model-free_(reinforcement_learning)" title="Model-free (reinforcement learning)">model-free</a> <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> methods which learn by <a href="/wiki/Bootstrapping_(statistics)" title="Bootstrapping (statistics)">bootstrapping</a> from the current estimate of the value function. These methods sample from the environment, like <a href="/wiki/Monte_Carlo_method" title="Monte Carlo method">Monte Carlo methods</a>, and perform updates based on current estimates, like <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> methods.<br/><br/>While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of <a href="/wiki/Bootstrapping" title="Bootstrapping">bootstrapping</a>, as illustrated with the following example: <b><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistical_model" title="Statistical model">statistical modeling</a>, <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a></b> is a set of statistical processes for <a href="/wiki/Estimation_theory" title="Estimation theory">estimating</a> the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a> and one or more <a class="mw-redirect" href="/wiki/Independent_variable" title="Independent variable">independent variables</a> (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable  (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.<br/><br/>Most commonly, regression analysis estimates the <a href="/wiki/Conditional_expectation" title="Conditional expectation">conditional expectation</a> of the dependent variable given the independent variables – that is, the <a class="mw-redirect" href="/wiki/Average_value" title="Average value">average value</a> of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a <a href="/wiki/Quantile" title="Quantile">quantile</a>, or other <a href="/wiki/Location_parameter" title="Location parameter">location parameter</a> of the conditional distribution of the dependent variable given the independent variables. In all cases, a <a href="/wiki/Function_(mathematics)" title="Function (mathematics)">function</a> of the independent variables called the <b>regression function</b> is to be estimated. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the  regression function using a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a>. A related but distinct approach is Necessary Condition Analysis (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is <a href="/wiki/Necessity_and_sufficiency" title="Necessity and sufficiency">necessary but not sufficient</a> for a given value of the dependent variable. <b><a href="/wiki/Regression_analysis" title="Regression analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:GaussianScatterPCA.svg"><img alt="" class="thumbimage" data-file-height="720" data-file-width="720" decoding="async" height="220" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/330px-GaussianScatterPCA.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/440px-GaussianScatterPCA.svg.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:GaussianScatterPCA.svg" title="Enlarge"></a></div>PCA of a <a class="mw-redirect" href="/wiki/Multivariate_Gaussian_distribution" title="Multivariate Gaussian distribution">multivariate Gaussian distribution</a> centered at (1,3) with a standard deviation of 3 in roughly the (0.866, 0.5) direction and of 1 in the orthogonal direction. The vectors shown are the <a href="/wiki/Eigenvalues_and_eigenvectors" title="Eigenvalues and eigenvectors">eigenvectors</a> of the <a href="/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a> scaled by the square root of the corresponding eigenvalue, and shifted so their tails are at the mean.</div></div></div><br/><br/><b><a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></b> (<b>PCA</b>) is a statistical procedure that uses an <a href="/wiki/Orthogonal_transformation" title="Orthogonal transformation">orthogonal transformation</a> to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of <a href="/wiki/Correlation_and_dependence" title="Correlation and dependence">linearly uncorrelated</a> variables called <b>principal components</b>. If there are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n}</annotation>
</semantics>
</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span> observations with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> variables, then the number of distinct principal components is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \min(n-1,p)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo form="prefix" movablelimits="true">min</mo>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo>,</mo>
<mi>p</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \min(n-1,p)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \min(n-1,p)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/050538805fc5cf2f3205b2b758cba07bc6befc18" style="vertical-align: -0.838ex; width:13.285ex; height:2.843ex;"/></span>. This transformation is defined in such a way that the first principal component has the largest possible <a href="/wiki/Variance" title="Variance">variance</a> (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is <a class="mw-redirect" href="/wiki/Orthogonal" title="Orthogonal">orthogonal</a> to the preceding components. The resulting vectors (each being a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of the variables and containing <i>n</i> observations) are an uncorrelated <a class="mw-redirect" href="/wiki/Orthogonal_basis_set" title="Orthogonal basis set">orthogonal basis set</a>. PCA is sensitive to the relative scaling of the original variables.<br/><br/>PCA was invented in 1901 by <a href="/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a>, as an analogue of the <a href="/wiki/Principal_axis_theorem" title="Principal axis theorem">principal axis theorem</a> in mechanics; it was later independently developed and named by <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Harold Hotelling</a> in the 1930s. Depending on the field of application, it is also named the discrete <a href="/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem" title="Karhunen–Loève theorem">Karhunen–Loève</a> transform (KLT) in <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, the <a href="/wiki/Harold_Hotelling" title="Harold Hotelling">Hotelling</a> transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a> (SVD) of <b>X</b> (Golub and Van Loan, 1983), <a class="mw-redirect" href="/wiki/Eigendecomposition" title="Eigendecomposition">eigenvalue decomposition</a> (EVD) of <b>X</b><sup>T</sup><b>X</b> in linear algebra, <a href="/wiki/Factor_analysis" title="Factor analysis">factor analysis</a> (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's <i>Principal Component Analysis</i>), <a class="mw-redirect" href="/wiki/Eckart%E2%80%93Young_theorem" title="Eckart–Young theorem">Eckart–Young theorem</a> (Harman, 1960), or  <a href="/wiki/Empirical_orthogonal_functions" title="Empirical orthogonal functions">empirical orthogonal functions</a> (EOF) in meteorological science, <a class="new" href="/w/index.php?title=Empirical_eigenfunction_decomposition&amp;action=edit&amp;redlink=1" title="Empirical eigenfunction decomposition (page does not exist)">empirical eigenfunction decomposition</a> (Sirovich, 1987), <a class="new" href="/w/index.php?title=Empirical_component_analysis&amp;action=edit&amp;redlink=1" title="Empirical component analysis (page does not exist)">empirical component analysis</a> (Lorenz, 1956), <a class="new" href="/w/index.php?title=Quasiharmonic_modes&amp;action=edit&amp;redlink=1" title="Quasiharmonic modes (page does not exist)">quasiharmonic modes</a> (Brooks et al., 1988), <a href="/wiki/Spectral_theorem" title="Spectral theorem">spectral decomposition</a> in noise and vibration, and <a class="mw-redirect" href="/wiki/Mode_shape" title="Mode shape">empirical modal analysis</a> in structural dynamics. <b><a href="/wiki/Principal_component_analysis" title="Principal component analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></b> is a <a href="/wiki/Statistics" title="Statistics">statistical</a> method used to describe <a href="/wiki/Variance" title="Variance">variability</a> among observed, correlated <a href="/wiki/Variable_(mathematics)" title="Variable (mathematics)">variables</a> in terms of a potentially lower number of unobserved variables called <b>factors</b>. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>. The observed variables are modelled as <a href="/wiki/Linear_combination" title="Linear combination">linear combinations</a> of the potential factors, plus "<a class="mw-redirect" href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a>" terms. Factor analysis aims to find independent latent variables.<br/><br/>It is a theory used in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and related to <a href="/wiki/Data_mining" title="Data mining">data mining</a>. The theory behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in biology, <a href="/wiki/Psychometrics" title="Psychometrics">psychometrics</a>, <a href="/wiki/Personality" title="Personality">personality</a> theories, <a href="/wiki/Marketing" title="Marketing">marketing</a>, <a href="/wiki/Product_management" title="Product management">product management</a>, <a href="/wiki/Operations_research" title="Operations research">operations research</a>, and <a href="/wiki/Finance" title="Finance">finance</a>. Proponents of factor analysis believe that it helps to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality. <b><a href="/wiki/Factor_analysis" title="Factor analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:402px;"><a class="image" href="/wiki/File:NMF.png"><img alt="" class="thumbimage" data-file-height="168" data-file-width="673" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/NMF.png/400px-NMF.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f9/NMF.png/600px-NMF.png 1.5x, //upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png 2x" width="400"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:NMF.png" title="Enlarge"></a></div>Illustration of approximate non-negative matrix factorization: the matrix <span class="texhtml"><b>V</b></span> is represented by the two smaller matrices <span class="texhtml"><b>W</b></span> and <span class="texhtml"><b>H</b></span>, which, when multiplied, approximately reconstruct <span class="texhtml"><b>V</b></span>.</div></div></div><br/><br/><b><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a></b> (<b>NMF</b> or <b>NNMF</b>), also <b>non-negative matrix approximation</b> is a group of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> in <a href="/wiki/Multivariate_analysis" title="Multivariate analysis">multivariate analysis</a> and <a href="/wiki/Linear_algebra" title="Linear algebra">linear algebra</a> where a <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> <span class="texhtml"><b>V</b></span> is <a href="/wiki/Matrix_decomposition" title="Matrix decomposition">factorized</a> into (usually) two matrices <span class="texhtml"><b>W</b></span> and <span class="texhtml"><b>H</b></span>, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.<br/><br/>NMF finds applications in such fields as <a href="/wiki/Astronomy" title="Astronomy">astronomy</a>, <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, document <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a>, <a href="/wiki/Chemometrics" title="Chemometrics">chemometrics</a>, <a href="/wiki/Audio_signal_processing" title="Audio signal processing">audio signal processing</a>, <a href="/wiki/Recommender_system" title="Recommender system">recommender systems</a>, and <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>. <b><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:172px;"><a class="image" href="/wiki/File:The_LSTM_cell.png"><img alt="" class="thumbimage" data-file-height="1322" data-file-width="2014" decoding="async" height="112" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/170px-The_LSTM_cell.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/255px-The_LSTM_cell.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/340px-The_LSTM_cell.png 2x" width="170"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:The_LSTM_cell.png" title="Enlarge"></a></div>The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.</div></div></div><br/><br/><b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory</a></b> (<b>LSTM</b>) is an artificial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) architecture used in the field of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>. Unlike standard <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>, LSTM has feedback connections that make it a "general purpose computer" (that is, it can compute anything that a <a href="/wiki/Turing_machine" title="Turing machine">Turing machine</a> can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<br/><a class="mw-redirect" href="/wiki/Bloomberg_Business_Week" title="Bloomberg Business Week">Bloomberg Business Week</a> wrote: "These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music."<br/><br/>A common LSTM unit is composed of a <b>cell</b>, an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. The cell remembers values over arbitrary time intervals and the three <i>gates</i> regulate the flow of information into and out of the cell. <b><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Mathematics" title="Mathematics">mathematics</a>, a <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance Vector Machine (RVM)</a></b> is a <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> technique that uses <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a> to obtain <a href="/wiki/Occam%27s_razor" title="Occam's razor">parsimonious</a> solutions for <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and <a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic classification</a>.<br/>The RVM has an identical functional form to the <a class="mw-redirect" href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a>, but provides probabilistic classification.<br/><br/>It is actually equivalent to a <a href="/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a> model with <a href="/wiki/Covariance_function" title="Covariance function">covariance function</a>:<br/>:<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>N</mi>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<msub>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mfrac>
</mrow>
<mi>φ<!-- φ --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mi>φ<!-- φ --></mi>
<mo stretchy="false">(</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>′</mo>
</msup>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}</annotation>
</semantics>
</math></span><img alt="k(\mathbf{x},\mathbf{x'}) = \sum_{j=1}^N \frac{1}{\alpha_j} \varphi(\mathbf{x},\mathbf{x}_j)\varphi(\mathbf{x}',\mathbf{x}_j) " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a9b6842d3fb3de1fc0b5cd93878ce056ca7997f" style="vertical-align: -3.338ex; width:34.51ex; height:7.676ex;"/></span><br/>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varphi }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>φ<!-- φ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varphi }</annotation>
</semantics>
</math></span><img alt="\varphi " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33ee699558d09cf9d653f6351f9fda0b2f4aaa3e" style="vertical-align: -0.838ex; width:1.52ex; height:2.176ex;"/></span> is the <a class="mw-redirect" href="/wiki/Kernel_function" title="Kernel function">kernel function</a> (usually Gaussian), <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \alpha _{j}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \alpha _{j}}</annotation>
</semantics>
</math></span><img alt="\alpha _{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/293a364991ab1ee55c25b0f60fd9e52af7b7dbde" style="vertical-align: -1.005ex; width:2.397ex; height:2.343ex;"/></span> are the variances of the prior on the weight vector<br/><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle w\sim N(0,\alpha ^{-1}I)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>w</mi>
<mo>∼<!-- ∼ --></mo>
<mi>N</mi>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<msup>
<mi>α<!-- α --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msup>
<mi>I</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle w\sim N(0,\alpha ^{-1}I)}</annotation>
</semantics>
</math></span><img alt="w \sim N(0,\alpha^{-1}I)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f4348b3f595d38d41a8dee315d073be107af558" style="vertical-align: -0.838ex; width:15.824ex; height:3.176ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>N</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}</annotation>
</semantics>
</math></span><img alt="\mathbf{x}_1,\ldots,\mathbf{x}_N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ceacccf86726a1ba6acfd40e67538e3e7c2336ec" style="vertical-align: -0.671ex; width:10.746ex; height:2.009ex;"/></span> are the input vectors of the <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a>. <b><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></b> (ERM) is a principle in <a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">statistical learning theory</a> which defines a family of <a href="/wiki/Machine_learning" title="Machine learning">learning algorithms</a> and is used to give theoretical bounds on their performance. <b><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Online_machine_learning" title="Online machine learning">online machine learning</a></b> is a method of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of <a class="mw-redirect" href="/wiki/Out-of-core" title="Out-of-core">out-of-core</a> algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., <a href="/wiki/Stock_market_prediction" title="Stock market prediction">stock price prediction</a>.<br/>Online learning algorithms may be prone to <a href="/wiki/Catastrophic_interference" title="Catastrophic interference">catastrophic interference</a>, a problem that can be addressed by <a href="/wiki/Incremental_learning" title="Incremental learning">incremental learning</a> approaches. <b><a href="/wiki/Online_machine_learning" title="Online machine learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></b> or <b>machine-learned ranking</b> (MLR) is the application of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, typically <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a>, <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised</a> or <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>, in the construction of <a class="mw-redirect" href="/wiki/Ranking_function" title="Ranking function">ranking models</a> for <a href="/wiki/Information_retrieval" title="Information retrieval">information retrieval</a> systems. <a class="mw-redirect" href="/wiki/Training_data" title="Training data">Training data</a> consists of lists of items with some <a class="mw-redirect" href="/wiki/Partial_order" title="Partial order">partial order</a> specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a <a href="/wiki/Permutation" title="Permutation">permutation</a> of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense. <b><a href="/wiki/Learning_to_rank" title="Learning to rank">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Synapse_Self-Organizing_Map.png"><img alt="" class="thumbimage" data-file-height="713" data-file-width="759" decoding="async" height="282" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/300px-Synapse_Self-Organizing_Map.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/450px-Synapse_Self-Organizing_Map.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/70/Synapse_Self-Organizing_Map.png/600px-Synapse_Self-Organizing_Map.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Synapse_Self-Organizing_Map.png" title="Enlarge"></a></div>A self-organizing map showing <a href="/wiki/United_States_Congress" title="United States Congress">U.S. Congress</a> voting patterns. The input data was a table with a row for each member of Congress, and columns for certain votes containing each member's yes/no/abstain vote. The SOM algorithm arranged these members in a two-dimensional grid placing similar members closer together. <b>The first plot</b> shows the grouping when the data are split into two clusters. <b>The second plot</b> shows average distance to neighbours: larger distances are darker. <b>The third plot</b> predicts <a href="/wiki/Republican_Party_(United_States)" title="Republican Party (United States)">Republican</a> (red) or <a href="/wiki/Democratic_Party_(United_States)" title="Democratic Party (United States)">Democratic</a> (blue) party membership. <b>The other plots</b> each overlay the resulting map with predicted values on an input dimension: red means a predicted 'yes' vote on that bill, blue means a 'no' vote. The plot was created in <a href="/wiki/Peltarion_Synapse" title="Peltarion Synapse">Synapse</a>.</div></div></div><br/>A <b><a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a></b> (<b>SOM</b>) or <b>self-organizing feature map</b> (<b>SOFM</b>) is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> (ANN) that is trained using <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a <b>map</b>, and is therefore a method to do <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>. Self-organizing maps differ from other artificial neural networks as they apply <a href="/wiki/Competitive_learning" title="Competitive learning">competitive learning</a> as opposed to error-correction learning (such as <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> with <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>), and in the sense that they use a neighborhood function to preserve the <a href="/wiki/Topology" title="Topology">topological</a> properties of the input space.<br/><br/>This makes SOMs useful for <a href="/wiki/Scientific_visualization" title="Scientific visualization">visualization</a> by creating low-dimensional views of high-dimensional data, akin to <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>. The artificial neural network introduced by the <a href="/wiki/Finland" title="Finland">Finnish</a> professor <a href="/wiki/Teuvo_Kohonen" title="Teuvo Kohonen">Teuvo Kohonen</a> in the 1980s is sometimes called a <b>Kohonen map</b> or <b>network</b>. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and <a href="/wiki/Morphogenesis" title="Morphogenesis">morphogenesis</a> models dating back to <a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> in the  1950s. <b><a href="/wiki/Self-organizing_map" title="Self-organizing map">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">hierarchical clustering</a></b> (also called <b>hierarchical cluster analysis</b> or <b>HCA</b>) is a method of <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> which seeks to build a <a href="/wiki/Hierarchy" title="Hierarchy">hierarchy</a> of clusters. Strategies for hierarchical clustering generally fall into two types:<ul><li> <b>Agglomerative</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">bottom-up</a>" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li><li> <b>Divisive</b>: This is a "<a href="/wiki/Top-down_and_bottom-up_design" title="Top-down and bottom-up design">top-down</a>" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li></ul><br/><br/>In general, the merges and splits are determined in a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a> manner. The results of hierarchical clustering are usually presented in a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>. <b><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Automated_machine_learning" title="Automated machine learning">Automated machine learning</a></b> (<b>AutoML</b>) is the process of automating the end-to-end process of applying <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> to real-world problems. In a typical machine learning application, practitioners must apply the appropriate <a href="/wiki/Data_pre-processing" title="Data pre-processing">data pre-processing</a>, <a href="/wiki/Feature_engineering" title="Feature engineering">feature engineering</a>, <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>, and <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform <a href="/wiki/Algorithm_selection" title="Algorithm selection">algorithm selection</a> and <a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">hyperparameter optimization</a> to maximize the predictive performance of their final machine learning model. As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>-based solution to the ever-growing challenge of applying machine learning. Automating the end-to-end process of applying machine learning offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand. <b><a href="/wiki/Automated_machine_learning" title="Automated machine learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b><a href="/wiki/Feature_learning" title="Feature learning">feature learning</a></b> or <b>representation learning</b> is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual <a href="/wiki/Feature_engineering" title="Feature engineering">feature engineering</a> and allows a machine to both learn the features  and use them to perform  a specific task.<br/><br/>Feature learning is motivated by the fact that machine learning tasks such as <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. <b><a href="/wiki/Feature_learning" title="Feature learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b>Density-based spatial clustering of applications with noise</b> (<b><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></b>) is a <a class="mw-redirect" href="/wiki/Data_clustering" title="Data clustering">data clustering</a> algorithm proposed by Martin Ester, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a>, Jörg Sander and Xiaowei Xu in 1996.<br/>It is a <a href="/wiki/Cluster_analysis#Density-based_clustering" title="Cluster analysis">density-based clustering</a> algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many <a href="/wiki/Fixed-radius_near_neighbors" title="Fixed-radius near neighbors">nearby neighbors</a>), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).<br/>DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.<br/><br/>In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD. <b><a href="/wiki/DBSCAN" title="DBSCAN">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></b> (or <b>grammatical inference</b>) is the process in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> of learning a <a href="/wiki/Formal_grammar" title="Formal grammar">formal grammar</a> (usually as a collection of <i>re-write rules</i> or <i><a class="mw-redirect" href="/wiki/Productions_(computer_science)" title="Productions (computer science)">productions</a></i> or alternatively as a <a class="mw-redirect" href="/wiki/Finite_state_machine" title="Finite state machine">finite state machine</a> or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs. <b><a href="/wiki/Grammar_induction" title="Grammar induction">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">State–action–reward–state–action</a></b> (<b>SARSA</b>) is an <a href="/wiki/Algorithm" title="Algorithm">algorithm</a> for learning a <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> policy, used in the <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> area of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. It was proposed by Rummery and Niranjan in a technical note with the name "Modified Connectionist Q-Learning" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.<br/><br/>This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent "<b>S</b><sub>1</sub>", the action the agent chooses "<b>A</b><sub>1</sub>", the reward "<b>R</b>" the agent gets for choosing this action, the state "<b>S</b><sub>2</sub>" that the agent enters after taking that action, and finally the next action "<b>A</b><sub>2</sub>" the agent choose in its new state. The acronym for the quintuple (s<sub>t</sub>, a<sub>t</sub>, r<sub>t</sub>, s<sub>t+1</sub>, a<sub>t+1</sub>) is SARSA. <b><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a class="image" href="/wiki/File:Kernel_Machine.svg"><img alt="Kernel Machine.svg" data-file-height="233" data-file-width="512" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" width="220"/></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-nearest neighbors algorithm</a></b> (<b><i>k</i>-NN</b>) is a <a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">non-parametric</a> method used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. In both cases, the input consists of the <i>k</i> closest training examples in the <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:<br/><br/>:* In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i> = 1, then the object is simply assigned to the class of that single nearest neighbor. <b><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bootstrap aggregating</a></b>, also called <b>bagging</b>, is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a class="mw-redirect" href="/wiki/Meta-algorithm" title="Meta-algorithm">meta-algorithm</a> designed to improve the stability and accuracy of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms used in <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>. It also reduces <a href="/wiki/Variance" title="Variance">variance</a> and helps to avoid <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. Although it is usually applied to <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision tree</a> methods, it can be used with any type of method. Bagging is a special case of the <a href="/wiki/Ensemble_learning" title="Ensemble learning">model averaging</a> approach. <b><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">A <b><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a></b> (<b>RNN</b>) is a class of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> where connections between nodes form a <a href="/wiki/Directed_graph" title="Directed graph">directed graph</a> along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike <a class="mw-redirect" href="/wiki/Feedforward_neural_networks" title="Feedforward neural networks">feedforward neural networks</a>, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a> or <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>.<br/><br/>The term "recurrent neural network" is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is <a href="/wiki/Finite_impulse_response" title="Finite impulse response">finite impulse</a> and the other is <a href="/wiki/Infinite_impulse_response" title="Infinite impulse response">infinite impulse</a>. Both classes of networks exhibit temporal <a class="mw-redirect" href="/wiki/Dynamic_system" title="Dynamic system">dynamic behavior</a>. A finite impulse recurrent network is a <a href="/wiki/Directed_acyclic_graph" title="Directed acyclic graph">directed acyclic graph</a> that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a <a class="mw-redirect" href="/wiki/Directed_cyclic_graph" title="Directed cyclic graph">directed cyclic graph</a> that can not be unrolled. <b><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:362px;"><a class="image" href="/wiki/File:EM_Clustering_of_Old_Faithful_data.gif"><img alt="" class="thumbimage" data-file-height="309" data-file-width="360" decoding="async" height="309" src="//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif" width="360"/></a> <div class="thumbcaption">EM clustering of <a href="/wiki/Old_Faithful" title="Old Faithful">Old Faithful</a> eruption data. The random initial model (which, due to the different scales of the axes, appears to be two very flat and wide spheres) is fit to the observed data. In the first iterations, the model changes substantially, but then converges to the two modes of the <a href="/wiki/Geyser" title="Geyser">geyser</a>. Visualized using <a href="/wiki/ELKI" title="ELKI">ELKI</a>.</div></div></div><br/>In <a href="/wiki/Statistics" title="Statistics">statistics</a>, an <b><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">expectation–maximization</a></b> (<b>EM</b>) <b>algorithm</b> is an <a href="/wiki/Iterative_method" title="Iterative method">iterative method</a> to find <a class="mw-redirect" href="/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood</a> or <a class="mw-redirect" href="/wiki/Maximum_a_posteriori" title="Maximum a posteriori">maximum a posteriori</a> (MAP) estimates of <a href="/wiki/Parameter" title="Parameter">parameters</a> in <a href="/wiki/Statistical_model" title="Statistical model">statistical models</a>, where the model depends on unobserved <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the <a href="/wiki/Likelihood_function#Log-likelihood" title="Likelihood function">log-likelihood</a> evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the <i>E</i> step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. <b><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random fields</a></b> (<b>CRFs</b>) are a class of <a href="/wiki/Statistical_model" title="Statistical model">statistical modeling method</a> often applied in <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and used for <a href="/wiki/Structured_prediction" title="Structured prediction">structured prediction</a>. CRFs fall into the sequence modeling family. Whereas a discrete <a href="/wiki/Statistical_classification" title="Statistical classification">classifier</a> predicts a label for a single sample without considering "neighboring" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>) predicts sequences of labels for sequences of input samples.<br/><br/>CRFs are a type of <a href="/wiki/Discriminative_model" title="Discriminative model">discriminative</a> <a href="/wiki/Markov_random_field" title="Markov random field">undirected</a> <a href="/wiki/Statistical_model" title="Statistical model">probabilistic</a> <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a>. They are used to encode known relationships between observations and construct consistent interpretations and are often used for <a href="/wiki/Sequence_labeling" title="Sequence labeling">labeling</a> or <a href="/wiki/Parsing" title="Parsing">parsing</a> of sequential data, such as natural language processing or <a href="/wiki/Bioinformatics" title="Bioinformatics">biological sequences</a><br/>and in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>.<br/>Specifically, CRFs find applications in POS Tagging, <a href="/wiki/Shallow_parsing" title="Shallow parsing">shallow parsing</a>,<br/><a class="mw-redirect" href="/wiki/Named_entity_recognition" title="Named entity recognition">named entity recognition</a>,<br/><a href="/wiki/Gene_prediction" title="Gene prediction">gene finding</a> and peptide critical functional region finding,<br/>among other tasks, being an alternative to the related <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">hidden Markov models</a> (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation. <b><a href="/wiki/Conditional_random_field" title="Conditional random field">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b><a href="/wiki/Perceptron" title="Perceptron">perceptron</a></b> is an algorithm for <a class="mw-redirect" href="/wiki/Supervised_classification" title="Supervised classification">supervised learning</a> of <a href="/wiki/Binary_classification" title="Binary classification">binary classifiers</a>.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, i.e. a classification algorithm that makes its predictions based on a <a href="/wiki/Linear_predictor_function" title="Linear predictor function">linear predictor function</a> combining a set of weights with the <a class="mw-redirect" href="/wiki/Feature_vector" title="Feature vector">feature vector</a>. <b><a href="/wiki/Perceptron" title="Perceptron">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">These <a href="/wiki/Data_set" title="Data set">datasets</a> are used for <a class="mw-redirect" href="/wiki/Machine-learning" title="Machine-learning">machine-learning</a> research and have been cited in <a href="/wiki/Peer_review" title="Peer review">peer-reviewed</a> academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> (such as <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> and <a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-supervised</a> machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> learning can also be difficult and costly to produce. <b><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></b> (<b>LDA</b>), <b>normal discriminant analysis</b> (<b>NDA</b>), or <b>discriminant function analysis</b> is a generalization of <b>Fisher's linear discriminant</b>, a method used in <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> to find a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of <a class="mw-redirect" href="/wiki/Features_(pattern_recognition)" title="Features (pattern recognition)">features</a> that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a <a href="/wiki/Linear_classifier" title="Linear classifier">linear classifier</a>, or, more commonly, for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> before later <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>.<br/><br/>LDA is closely related to <a href="/wiki/Analysis_of_variance" title="Analysis of variance">analysis of variance</a> (ANOVA) and <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, which also attempt to express one <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a> as a linear combination of other features or measurements. However, ANOVA uses <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> <a class="mw-redirect" href="/wiki/Independent_variables" title="Independent variables">independent variables</a> and a <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous</a> <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a>, whereas discriminant analysis has continuous <a class="mw-redirect" href="/wiki/Independent_variables" title="Independent variables">independent variables</a> and a categorical dependent variable (<i>i.e.</i> the class label). <a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a> and <a class="mw-redirect" href="/wiki/Probit_regression" title="Probit regression">probit regression</a> are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. <b><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></b> is a <a class="mw-redirect" href="/wiki/Non-parametric" title="Non-parametric">non-parametric</a> <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature-space</a> analysis technique for locating the maxima of a <a class="mw-redirect" href="/wiki/Density_function" title="Density function">density function</a>, a so-called <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a>-seeking algorithm. Application domains include <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> in <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> and <a class="mw-redirect" href="/wiki/Image_processing" title="Image processing">image processing</a>. <b><a href="/wiki/Mean_shift" title="Mean shift">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Statistics" title="Statistics">statistics</a>, the <b>logistic model</b> (or <b>logit model</b>) is a widely used <a href="/wiki/Statistical_model" title="Statistical model">statistical model</a> that, in its basic form, uses a <a href="/wiki/Logistic_function" title="Logistic function">logistic function</a> to model a <a class="mw-redirect" href="/wiki/Binary_variable" title="Binary variable">binary</a> <a class="mw-redirect" href="/wiki/Dependent_variable" title="Dependent variable">dependent variable</a>; many more complex <a href="#Extensions">extensions</a> exist. In <a href="/wiki/Regression_analysis" title="Regression analysis">regression analysis</a>, <b><a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a></b> (or <b>logit regression</b>) is <a href="/wiki/Estimation_theory" title="Estimation theory">estimating</a> the parameters of a logistic model; it is a form of <a href="/wiki/Binomial_regression" title="Binomial regression">binomial regression</a>. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail, win/lose, alive/dead or healthy/sick; these are represented by an <a class="mw-redirect" href="/wiki/Indicator_variable" title="Indicator variable">indicator variable</a>, where the two values are labeled "0" and "1". In the logistic model, the <a class="mw-redirect" href="/wiki/Log-odds" title="Log-odds">log-odds</a> (the <a href="/wiki/Logarithm" title="Logarithm">logarithm</a> of the <a href="/wiki/Odds" title="Odds">odds</a>) for the value labeled "1" is a <a href="/wiki/Linear_function_(calculus)" title="Linear function (calculus)">linear combination</a> of one or more <a class="mw-redirect" href="/wiki/Independent_variable" title="Independent variable">independent variables</a> ("predictors"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous variable</a> (any real value). The corresponding <a href="/wiki/Probability" title="Probability">probability</a> of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The <a href="/wiki/Unit_of_measurement" title="Unit of measurement">unit of measurement</a> for the log-odds scale is called a <i><a href="/wiki/Logit" title="Logit">logit</a></i>, from <i><b>log</b>istic un<b>it</b></i>, hence the alternative names. Analogous models with a different <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> instead of the logistic function can also be used, such as the <a href="/wiki/Probit_model" title="Probit model">probit model</a>; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a <i>constant</i> rate, with each dependent variable having its own parameter; for a binary independent variable this generalizes the <a href="/wiki/Odds_ratio" title="Odds ratio">odds ratio</a>.<br/><br/>Logistic regression was developed by statistician <a href="/wiki/David_Cox_(statistician)" title="David Cox (statistician)">David Cox</a> in 1958. The binary logistic regression model has <a href="#Extensions">extensions</a> to more than two levels of the dependent variable: <a href="/wiki/Categorical_variable" title="Categorical variable">categorical</a> outputs with more than two values are modelled by <a href="/wiki/Multinomial_logistic_regression" title="Multinomial logistic regression">multinomial logistic regression</a>, and if the multiple categories are <a href="/wiki/Level_of_measurement#Ordinal_type" title="Level of measurement">ordered</a>, by <a class="mw-redirect" href="/wiki/Ordinal_logistic_regression" title="Ordinal logistic regression">ordinal logistic regression</a>, for example the proportional odds ordinal logistic model. The model itself simply models probability of output in terms of input, and does not perform <a href="/wiki/Statistical_classification" title="Statistical classification">statistical classification</a> (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a <a class="mw-redirect" href="/wiki/Binary_classifier" title="Binary classifier">binary classifier</a>. The coefficients are generally not computed by a closed-form expression, unlike <a class="mw-redirect" href="/wiki/Linear_least_squares_(mathematics)" title="Linear least squares (mathematics)">linear least squares</a>; see <a href="#Model_fitting">§ Model fitting</a>. <b><a href="/wiki/Logistic_regression" title="Logistic regression">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">Ordering points to identify the clustering structure</a></b> (<b>OPTICS</b>) is an algorithm for finding density-based <a href="/wiki/Cluster_analysis" title="Cluster analysis">clusters</a> in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a> and Jörg Sander.<br/>Its basic idea is similar to <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a>, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>. <b><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rule learning</a></b> is a <a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">rule-based machine learning</a> method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. This rule-based approach also generates new rules as it analyzes more data. The ultimate goal, assuming a large enough dataset, is to help a machine mimic the human brain’s <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a> and <a href="/wiki/Abstraction" title="Abstraction">abstract association</a> capabilities from new uncategorized data.<br/><br/>Based on the concept of strong rules, <a href="/wiki/Rakesh_Agrawal_(computer_scientist)" title="Rakesh Agrawal (computer scientist)">Rakesh Agrawal</a>, <a href="/wiki/Tomasz_Imieli%C5%84ski" title="Tomasz Imieliński">Tomasz Imieliński</a> and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by <a class="mw-redirect" href="/wiki/Point-of-sale" title="Point-of-sale">point-of-sale</a> (POS) systems in supermarkets. For example, the rule <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">{</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">o</mi>
<mi mathvariant="normal">n</mi>
<mi mathvariant="normal">i</mi>
<mi mathvariant="normal">o</mi>
<mi mathvariant="normal">n</mi>
<mi mathvariant="normal">s</mi>
<mo>,</mo>
<mi mathvariant="normal">p</mi>
<mi mathvariant="normal">o</mi>
<mi mathvariant="normal">t</mi>
<mi mathvariant="normal">a</mi>
<mi mathvariant="normal">t</mi>
<mi mathvariant="normal">o</mi>
<mi mathvariant="normal">e</mi>
<mi mathvariant="normal">s</mi>
</mrow>
<mo fence="false" stretchy="false">}</mo>
<mo stretchy="false">⇒<!-- ⇒ --></mo>
<mo fence="false" stretchy="false">{</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">b</mi>
<mi mathvariant="normal">u</mi>
<mi mathvariant="normal">r</mi>
<mi mathvariant="normal">g</mi>
<mi mathvariant="normal">e</mi>
<mi mathvariant="normal">r</mi>
</mrow>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}</annotation>
</semantics>
</math></span><img alt="\{{\mathrm  {onions,potatoes}}\}\Rightarrow \{{\mathrm  {burger}}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2e6daa2c8e553e87e411d6e0ec66ae596c3c9381" style="vertical-align: -0.838ex; width:30.912ex; height:2.843ex;"/></span> found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional <a href="/wiki/Pricing" title="Pricing">pricing</a> or <a href="/wiki/Product_placement" title="Product placement">product placements</a>. <b><a href="/wiki/Association_rule_learning" title="Association rule learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Test_function_and_noisy_data.png"><img alt="" class="thumbimage" data-file-height="901" data-file-width="1201" decoding="async" height="165" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/220px-Test_function_and_noisy_data.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/330px-Test_function_and_noisy_data.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/440px-Test_function_and_noisy_data.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Test_function_and_noisy_data.png" title="Enlarge"></a></div>Function and noisy data.</div></div></div><br/>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">bias–variance tradeoff</a></b> is the property of a set of predictive models whereby models with a lower <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">bias</a> in <a href="/wiki/Statistical_parameter" title="Statistical parameter">parameter</a> <a href="/wiki/Estimation_theory" title="Estimation theory">estimation</a> have a higher <a href="/wiki/Variance" title="Variance">variance</a> of the parameter estimates across <a href="/wiki/Sample_(statistics)" title="Sample (statistics)">samples</a>, and vice versa. The <b>bias–variance dilemma</b> or <b>problem</b> is the conflict in trying to simultaneously minimize these two sources of <a class="mw-redirect" href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a> that prevent <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> algorithms from generalizing beyond their <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a>:<ul><li> The <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator"><i>bias</i></a> is an error from erroneous assumptions in the learning <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li><li> The <i><a href="/wiki/Variance" title="Variance">variance</a></i> is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random <a href="/wiki/Noise_(signal_processing)" title="Noise (signal processing)">noise</a> in the training data, rather than the intended outputs (<a href="/wiki/Overfitting" title="Overfitting">overfitting</a>).</li></ul><br/><br/>The <b>bias–variance decomposition</b> is a way of analyzing a learning algorithm's <a href="/wiki/Expected_value" title="Expected value">expected</a> <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the <i>irreducible error</i>, resulting from noise in the problem itself. <b><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Data_mining" title="Data mining">data mining</a>, <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a></b> (also <b>outlier detection</b>) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as <a href="/wiki/Bank_fraud" title="Bank fraud">bank fraud</a>, a structural defect, medical problems or errors in a text. Anomalies are also referred to as <a href="/wiki/Outlier" title="Outlier">outliers</a>, novelties, noise, deviations and exceptions.<br/><br/>In particular, in the context of abuse and network intrusion detection, the interesting objects are often not <i>rare</i> objects, but unexpected <i>bursts</i> in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> algorithm may be able to detect the micro clusters formed by these patterns. <b><a href="/wiki/Anomaly_detection" title="Anomaly detection">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></b> is a <a href="/wiki/Ensemble_learning" title="Ensemble learning">machine learning ensemble</a> <a class="mw-redirect" href="/wiki/Meta-algorithm" title="Meta-algorithm">meta-algorithm</a> for primarily reducing <a href="/wiki/Supervised_learning#Bias-variance_tradeoff" title="Supervised learning">bias</a>, and also variance in <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by <a href="/wiki/Michael_Kearns_(computer_scientist)" title="Michael Kearns (computer scientist)">Kearns</a> and <a href="/wiki/Leslie_Valiant" title="Leslie Valiant">Valiant</a> (1988, 1989): "Can a set of <b>weak learners</b> create a single <b>strong learner</b>?" A weak learner is defined to be a <a class="mw-redirect" href="/wiki/Classification_(machine_learning)" title="Classification (machine learning)">classifier</a> that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.<br/><br/><a href="/wiki/Robert_Schapire" title="Robert Schapire">Robert Schapire</a>'s affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, most notably leading to the development of boosting. <b><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Random_forest" title="Random forest">Random forests</a></b> or <b>random decision forests</b> are an <a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble learning</a> method for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>, <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and other tasks that operates by constructing a multitude of <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision trees</a> at training time and outputting the class that is the <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a> of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of <a href="/wiki/Overfitting" title="Overfitting">overfitting</a> to their <a class="mw-redirect" href="/wiki/Test_set" title="Test set">training set</a>.<br/><br/>The first algorithm for random decision forests was created by <a href="/wiki/Tin_Kam_Ho" title="Tin Kam Ho">Tin Kam Ho</a> using the <a href="/wiki/Random_subspace_method" title="Random subspace method">random subspace method</a>, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg. <b><a href="/wiki/Random_forest" title="Random forest">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></b> (<b>RL</b>) is an area of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> concerned with how <a href="/wiki/Software_agent" title="Software agent">software agents</a> ought to take <i><a href="/wiki/Action_selection" title="Action selection">actions</a></i> in an <i>environment</i> so as to maximize some notion of cumulative <i>reward</i>. The problem, due to its generality, is studied in many other disciplines, such as <a href="/wiki/Game_theory" title="Game theory">game theory</a>, <a href="/wiki/Control_theory" title="Control theory">control theory</a>, <a href="/wiki/Operations_research" title="Operations research">operations research</a>, <a href="/wiki/Information_theory" title="Information theory">information theory</a>, <a href="/wiki/Simulation-based_optimization" title="Simulation-based optimization">simulation-based optimization</a>, <a href="/wiki/Multi-agent_system" title="Multi-agent system">multi-agent systems</a>, <a href="/wiki/Swarm_intelligence" title="Swarm intelligence">swarm intelligence</a>, <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Genetic_algorithm" title="Genetic algorithm">genetic algorithms</a>. In the operations research and control literature, reinforcement learning is called <i>approximate dynamic programming,</i> or <i>neuro-dynamic programming.</i><br/>The problems of interest in reinforcement learning have also been studied in the <a class="mw-redirect" href="/wiki/Optimal_control_theory" title="Optimal control theory">theory of optimal control</a>, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In <a href="/wiki/Economics" title="Economics">economics</a> and <a href="/wiki/Game_theory" title="Game theory">game theory</a>, reinforcement learning may be used to explain how equilibrium may arise under <a href="/wiki/Bounded_rationality" title="Bounded rationality">bounded rationality</a>.<br/>In machine learning, the environment is typically formulated as a <a class="mw-redirect" href="/wiki/Markov_Decision_Process" title="Markov Decision Process">Markov Decision Process</a> (MDP), as many reinforcement learning algorithms for this context utilize <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.<br/><br/>Reinforcement learning is considered as one of three machine learning paradigms, alongside <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> and <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>. It differs from supervised learning in that correct input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between <a href="/wiki/Exploration" title="Exploration">exploration</a> (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off has been most thoroughly studied through the <a href="/wiki/Multi-armed_bandit" title="Multi-armed bandit">multi-armed bandit</a> problem and in finite MDPs. <b><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Signal_processing" title="Signal processing">signal processing</a>, <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">independent component analysis</a></b> (<b>ICA</b>) is a computational method for separating a <a href="/wiki/Multivariate_statistics" title="Multivariate statistics">multivariate</a> signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are <a class="mw-redirect" href="/wiki/Statistical_independence" title="Statistical independence">statistically independent</a> from each other. ICA is a special case of <a class="mw-redirect" href="/wiki/Blind_source_separation" title="Blind source separation">blind source separation</a>. A common example application is the "<a class="mw-redirect" href="/wiki/Cocktail_party_problem" title="Cocktail party problem">cocktail party problem</a>" of listening in on one person's speech in a noisy room. <b><a href="/wiki/Independent_component_analysis" title="Independent component analysis">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>, the <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a></b> (<b>LOF</b>) is an algorithm proposed by Markus M. Breunig, <a href="/wiki/Hans-Peter_Kriegel" title="Hans-Peter Kriegel">Hans-Peter Kriegel</a>, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.<br/><br/>LOF shares some concepts with <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a> and <a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a> such as the concepts of "core distance" and "reachability distance", which are used for local density estimation. <b><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:196px;"><a class="image" href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png"><img alt="" class="thumbimage" data-file-height="449" data-file-width="388" decoding="async" height="225" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/194px-Example_of_unlabeled_data_in_semisupervised_learning.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png/291px-Example_of_unlabeled_data_in_semisupervised_learning.png 1.5x, //upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png 2x" width="194"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Example_of_unlabeled_data_in_semisupervised_learning.png" title="Enlarge"></a></div>An example of the influence of unlabeled data in semi-supervised learning.  The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example.  The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles).  This could be viewed as performing <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a> and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.</div></div></div><br/><br/><b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></b> is a class of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> tasks and techniques that also make use of unlabeled <a href="/wiki/Data" title="Data">data</a> for training – typically a small amount of <a href="/wiki/Labeled_data" title="Labeled data">labeled data</a> with a large amount of unlabeled data.  Semi-supervised learning falls between <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> (without any labeled training data) and <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> (with completely labeled training data).  Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy over unsupervised learning (where no data is labeled), but without the time and costs needed for supervised learning (where all data is labeled). The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value.  Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.<br/><br/>As in the supervised learning framework, we are given a set of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle l}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>l</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle l}</annotation>
</semantics>
</math></span><img alt="l" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/829091f745070b9eb97a80244129025440a1cfac" style="vertical-align: -0.338ex; width:0.693ex; height:2.176ex;"/></span> <a class="mw-redirect" href="/wiki/Independent_identically_distributed" title="Independent identically distributed">independently identically distributed</a> examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{1},\dots ,x_{l}\in X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{1},\dots ,x_{l}\in X}</annotation>
</semantics>
</math></span><img alt="x_{1},\dots ,x_{l}\in X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76da26bfd12e40809f4b2dae37ecca34ad1c825c" style="vertical-align: -0.671ex; width:14.435ex; height:2.509ex;"/></span> with corresponding labels <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y_{1},\dots ,y_{l}\in Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y_{1},\dots ,y_{l}\in Y}</annotation>
</semantics>
</math></span><img alt="y_{1},\dots ,y_{l}\in Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b376abba952ea1dca784912adb9a3bfd006eb6b" style="vertical-align: -0.671ex; width:13.847ex; height:2.509ex;"/></span>.  Additionally, we are given <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle u}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>u</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle u}</annotation>
</semantics>
</math></span><img alt="u" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e6bb763d22c20916ed4f0bb6bd49d7470cffd8" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> unlabeled examples <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
<mo>+</mo>
<mi>u</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}</annotation>
</semantics>
</math></span><img alt="x_{l+1},\dots ,x_{l+u}\in X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60a05a61c90d36f1a9def946c7dd83e826162b26" style="vertical-align: -0.671ex; width:18.423ex; height:2.509ex;"/></span>.  Semi-supervised learning attempts to make use of this combined information to surpass the <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. <b><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Colored_neural_network.svg"><img alt="" class="thumbimage" data-file-height="356" data-file-width="296" decoding="async" height="361" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/450px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/600px-Colored_neural_network.svg.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Colored_neural_network.svg" title="Enlarge"></a></div>An artificial neural network is an interconnected group of nodes, similar to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.</div></div></div><br/><b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></b> (<b>ANN</b>) or <b>connectionist systems</b> are computing systems inspired by the <a class="mw-redirect" href="/wiki/Biological_neural_network" title="Biological neural network">biological neural networks</a> that constitute animal <a href="/wiki/Brain" title="Brain">brains</a>. The neural network itself is not an algorithm, but rather a framework for many different <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms to work together and process complex data inputs. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in <a class="mw-redirect" href="/wiki/Image_recognition" title="Image recognition">image recognition</a>, they might learn to identify images that contain cats by analyzing example images that have been manually <a href="/wiki/Labeled_data" title="Labeled data">labeled</a> as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.<br/><br/>An ANN is based on a collection of connected units or nodes called <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neurons</a>, which loosely model the <a href="/wiki/Neuron" title="Neuron">neurons</a> in a biological brain. Each connection, like the <a href="/wiki/Synapse" title="Synapse">synapses</a> in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. <b><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b><a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a></b> (or just <b>learning theory</b>) is a subfield of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> devoted to studying the design and analysis of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms. <b><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></b> is the process of using <a href="/wiki/Domain_knowledge" title="Domain knowledge">domain knowledge</a> of the data to create <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">features</a> that make <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>.<br/><br/>Feature engineering is an informal topic, but it is considered essential in applied machine learning. <b><a href="/wiki/Feature_engineering" title="Feature engineering">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;">The following <a href="/wiki/Outline_(list)" title="Outline (list)">outline</a> is provided as an overview of and topical guide to <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">machine learning</a></b>. <a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> is a subfield of <a href="/wiki/Soft_computing" title="Soft computing">soft computing</a> within <a href="/wiki/Computer_science" title="Computer science">computer science</a> that evolved from the study of <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a> and <a href="/wiki/Computational_learning_theory" title="Computational learning theory">computational learning theory</a> in <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>. In 1959, <a href="/wiki/Arthur_Samuel" title="Arthur Samuel">Arthur Samuel</a> defined machine learning as a "field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of <a href="/wiki/Algorithm" title="Algorithm">algorithms</a> that can <a href="/wiki/Learning" title="Learning">learn</a> from and make predictions on <a href="/wiki/Data" title="Data">data</a>. Such algorithms operate by building a <a href="/wiki/Mathematical_model" title="Mathematical model">model</a> from an example <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a> of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. <b><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Read more...</a></b></div>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:35px auto;"><a class="image" href="/wiki/File:Blank.png"><img alt="" data-file-height="2" data-file-width="3" decoding="async" height="80" src="//upload.wikimedia.org/wikipedia/commons/d/d2/Blank.png" width="120"/></a></div></div>
<div class="gallerytext">
<div style="text-align:left;"><b><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning theory</a></b> is a framework for <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a><br/>drawing from the fields of <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Functional_analysis" title="Functional analysis">functional analysis</a>. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>, <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, <a href="/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a> and <a href="/wiki/Baseball" title="Baseball">baseball</a>. <b><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Read more...</a></b></div>
</div>
</div></li>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span id="Need_help.3F"></span><span class="mw-headline" id="Need_help?">Need help?</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<p>Do you have a question about Machine learning that you can't find the answer to?
</p><p>Consider asking it at the <a href="/wiki/Wikipedia:Reference_desk" title="Wikipedia:Reference desk">Wikipedia reference desk</a>.
</p>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div><div class="flex-columns-column"><div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Selected_images">Selected images</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<style data-mw-deduplicate="TemplateStyles:r886046910">.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li.gallerycarousel>div>div>div>span:nth-child(2){display:none}@media screen and (max-width:720px){.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li:nth-child(n/**/+5){display:none}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow{padding-left:0;padding-right:0;display:flex;flex-wrap:wrap;justify-content:space-around;align-items:flex-start}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li{width:initial!important;margin:0 0.5em}.mw-parser-output div.randomSlideshow-container>ul.gallery.mw-gallery-slideshow>li>div>div>div{margin:0.5em 0!important}}</style><div class="randomSlideshow-container" style="max-width:100%; margin:-4em auto;"><ul class="gallery mw-gallery-slideshow" data-showthumbnails="">
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Svm_max_sep_hyperplane_with_margin.png"><img alt="" data-file-height="862" data-file-width="800" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/111px-Svm_max_sep_hyperplane_with_margin.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/167px-Svm_max_sep_hyperplane_with_margin.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/223px-Svm_max_sep_hyperplane_with_margin.png 2x" width="111"/></a></div></div>
<div class="gallerytext">
<p>A <a class="mw-redirect" href="/wiki/Support_vector_machine" title="Support vector machine">support vector machine</a> is a supervised learning model that divides the data into regions separated by a <a href="/wiki/Linear_classifier" title="Linear classifier">linear boundary</a>. Here, the linear boundary divides the black circles from the white.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 155px"><div style="width: 155px">
<div class="thumb" style="width: 150px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Colored_neural_network.svg"><img alt="" data-file-height="356" data-file-width="296" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/100px-Colored_neural_network.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/150px-Colored_neural_network.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/199px-Colored_neural_network.svg.png 2x" width="100"/></a></div></div>
<div class="gallerytext">
<p>An artificial neural network is an interconnected group of nodes, akin to the vast network of <a href="/wiki/Neuron" title="Neuron">neurons</a> in a <a href="/wiki/Brain" title="Brain">brain</a>. Here, each circular node represents an <a href="/wiki/Artificial_neuron" title="Artificial neuron">artificial neuron</a> and an arrow represents a connection from the output of one artificial neuron to the input of another.
</p>
</div>
</div></li>
</ul></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Subcategories">Subcategories</span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="floatright"><a class="image" href="/wiki/File:C_Puzzle.png" title="Category puzzle"><img alt="Category puzzle" data-file-height="150" data-file-width="150" decoding="async" height="36" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/36px-C_Puzzle.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/54px-C_Puzzle.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/da/C_Puzzle.png/72px-C_Puzzle.png 2x" width="36"/></a></div>
<dl><dd><small>Select [►] to view subcategories</small></dd></dl>
<div class="CategoryTreeTag" data-ct-mode="0" data-ct-options='{"mode":0,"hideprefix":20,"showcount":false,"namespaces":false}'><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-loaded="1" data-ct-state="expanded" data-ct-title="Machine_learning">▼</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></div><div class="CategoryTreeChildren" style="display:block"><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Applied_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Applied_machine_learning" title="Category:Applied machine learning">Applied machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Artificial_neural_networks">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Bayesian_networks" title="Category:Bayesian networks">Bayesian networks</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Classification_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Cluster_analysis">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Cluster_analysis" title="Category:Cluster analysis">Cluster analysis</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Computational_learning_theory" title="Category:Computational learning theory">Computational learning theory</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Artificial_intelligence_conferences" title="Category:Artificial intelligence conferences">Artificial intelligence conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Signal_processing_conferences" title="Category:Signal processing conferences">Signal processing conferences</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Data_mining_and_machine_learning_software">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Data_mining_and_machine_learning_software" title="Category:Data mining and machine learning software">Data mining and machine learning software</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Datasets_in_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Datasets_in_machine_learning" title="Category:Datasets in machine learning">Datasets in machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Deep_learning" title="Category:Deep learning">Deep learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Dimension_reduction">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ensemble_learning" title="Category:Ensemble learning">Ensemble learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Evolutionary_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Evolutionary_algorithms" title="Category:Evolutionary algorithms">Evolutionary algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Genetic_programming" title="Category:Genetic programming">Genetic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Inductive_logic_programming" title="Category:Inductive logic programming">Inductive logic programming</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Kernel_methods_for_machine_learning">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Kernel_methods_for_machine_learning" title="Category:Kernel methods for machine learning">Kernel methods for machine learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Latent_variable_models">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Latent_variable_models" title="Category:Latent variable models">Latent variable models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Learning_in_computer_vision" title="Category:Learning in computer vision">Learning in computer vision</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Log-linear_models" title="Category:Log-linear models">Log-linear models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Loss_functions" title="Category:Loss functions">Loss functions</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Machine_learning_algorithms">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_portal" title="Category:Machine learning portal">Machine learning portal</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_task" title="Category:Machine learning task">Machine learning task</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Markov_models">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Ontology_learning_(computer_science)" title="Category:Ontology learning (computer science)">Ontology learning (computer science)</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Machine_learning_researchers" title="Category:Machine learning researchers">Machine learning researchers</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Semisupervised_learning" title="Category:Semisupervised learning">Semisupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Statistical_natural_language_processing">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Statistical_natural_language_processing" title="Category:Statistical natural language processing">Statistical natural language processing</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeBullet"><span class="CategoryTreeToggle" data-ct-state="collapsed" data-ct-title="Structured_prediction">►</span> </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Structured_prediction" title="Category:Structured prediction">Structured prediction</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Supervised_learning" title="Category:Supervised learning">Supervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Support_vector_machines" title="Category:Support vector machines">Support vector machines</a></div><div class="CategoryTreeChildren" style="display:none"></div></div><div class="CategoryTreeSection"><div class="CategoryTreeItem"><span class="CategoryTreeEmptyBullet">► </span> <a class="CategoryTreeLabel CategoryTreeLabelNs14 CategoryTreeLabelCategory" href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></div><div class="CategoryTreeChildren" style="display:none"></div></div></div></div></div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div></div>
<div style="clear:both; width:100%">
<div class="box-header-title-container flex-columns-noflex" style="clear:both;color:#000000;margin-bottom:0px;border:solid #BFA2AE;box-sizing:border-box;text-align:center;font-size:100%;background:#D882A7;font-family:sans-serif;padding:.1em;border-width:1px 1px 0;padding-top:.1em;padding-left:.1em;padding-right:.1em;padding-bottom:.1em;moz-border-radius:0;webkit-border-radius:0;border-radius:0"><div class="plainlinks noprint" style="float:right;margin-bottom:.1em;color:#000000;font-size:80%"></div><h2 style="font-weight:bold;padding:0;margin:0;color:#000000;font-family:sans-serif;border:none;font-size:100%;padding-bottom:.1em"><span class="mw-headline" id="Associated_Wikimedia"><i>Associated Wikimedia</i></span></h2></div><div style="color:#000000;opacity:1;border:1px solid #BFA2AE;box-sizing:border-box;text-align:left;padding:1em;background:#FFF4F9;margin:0 0 10px;vertical-align:top;border-top-width:1px;padding-top:.3em;-moz-border-radius:0;-webkit-border-radius:0;border-radius:0">
<div class="noprint" style="text-align:center;">
<div style="display:inline-block;margin:0 0 0.75em 0">
<p style="margin:0">The following <a href="/wiki/Wikimedia_Foundation" title="Wikimedia Foundation">Wikimedia Foundation</a> sister projects provide more on this subject:</p>
<div style="display:inline-block;display:flex;flex-wrap:wrap;justify-content:center;"><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning" title="wikibooks:Special:Search/Machine learning">Wikibooks</a></b><br/>
Books<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikibooks.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="300" data-file-width="300" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/25px-Wikibooks-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/38px-Wikibooks-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikibooks-logo.svg/50px-Wikibooks-logo.svg.png 2x" width="25"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning" title="commons:Special:Search/Category:Machine learning">Commons</a></b><br/>
Media<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://commons.wikimedia.org/wiki/Special:Search/Category:Machine_learning"><img alt="" data-file-height="1376" data-file-width="1024" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/28px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/37px-Commons-logo.svg.png 2x" width="18"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikinews.org/wiki/Special:Search/Machine_learning" title="wikinews:Special:Search/Machine learning">Wikinews</a></b> <br/>
News<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikinews.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="415" data-file-width="759" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/46px-Wikinews-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/70px-Wikinews-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/24/Wikinews-logo.svg/92px-Wikinews-logo.svg.png 2x" width="46"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning" title="wikiquote:Special:Search/Machine learning">Wikiquote</a></b> <br/>
Quotations<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiquote.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="355" data-file-width="300" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/21px-Wikiquote-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/32px-Wikiquote-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/42px-Wikiquote-logo.svg.png 2x" width="21"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikisource.org/wiki/Special:Search/Machine_learning" title="wikisource:Special:Search/Machine learning">Wikisource</a></b> <br/>
Texts<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikisource.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="430" data-file-width="410" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/24px-Wikisource-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/36px-Wikisource-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/48px-Wikisource-logo.svg.png 2x" width="24"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning" title="wikiversity:Special:Search/Machine learning">Wikiversity</a></b><br/>
Learning resources<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wikiversity.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="800" data-file-width="1000" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/31px-Wikiversity-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/48px-Wikiversity-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/91/Wikiversity-logo.svg/63px-Wikiversity-logo.svg.png 2x" width="31"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning" title="wiktionary:Special:Search/Machine learning">Wiktionary</a></b> <br/>
Definitions<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://en.wiktionary.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="391" data-file-width="391" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/25px-Wiktionary-logo-v2.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/38px-Wiktionary-logo-v2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/0/06/Wiktionary-logo-v2.svg/50px-Wiktionary-logo-v2.svg.png 2x" width="25"/></a></div></div>
</div><div style="float:left; margin:0 3px 3px 3px; font-size:91%;">
<p><b><a class="extiw" href="https://www.wikidata.org/wiki/Special:Search/Machine_learning" title="wikidata:Special:Search/Machine learning">Wikidata</a></b> <br/>
Database<br/>
</p>
<div class="center"><div class="floatnone"><a href="https://www.wikidata.org/wiki/Special:Search/Machine_learning"><img alt="" data-file-height="590" data-file-width="1050" decoding="async" height="25" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/45px-Wikidata-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/68px-Wikidata-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/89px-Wikidata-logo.svg.png 2x" width="45"/></a></div></div>
</div>
</div>
</div>
</div>
<div class="noprint" style="margin:0.3em 0.2em 0.2em 0.3em; padding:0.3em 0.2em 0.2em 0.3em; text-align:right;"><b></b></div><div style="clear:both;"></div></div>
</div>
<div class="hlist noprint" style="text-align: center; clear:both; padding:0.25em 0 0.5em;">
<ul><li><b>What are <a href="/wiki/Wikipedia:Portal" title="Wikipedia:Portal">portals</a>?</b></li>
<li><b><a href="/wiki/Portal:Contents/Portals" title="Portal:Contents/Portals">List of portals</a></b></li></ul>
</div>
<p><i class="noprint plainlinks"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;action=purge"><small>Purge server cache</small></a></i>
</p></td></tr></tbody></table></div></div></div></li></ul></div></div></div></div></div>
<!-- 
NewPP limit report
Parsed by mw1254
Cached time: 20190315120211
Cache expiry: 21600
Dynamic content: true
CPU time usage: 4.376 seconds
Real time usage: 4.880 seconds
Preprocessor visited node count: 1163/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 70695/2097152 bytes
Template argument size: 1255/2097152 bytes
Highest expansion depth: 14/40
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 95382/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 4.105/10.000 seconds
Lua memory usage: 5.34 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 4629.634      1 -total
 96.51% 4467.870      1 Template:Flex_columns
 41.68% 1929.819      1 Template:Transclude_selected_recent_additions
 30.42% 1408.510      1 Template:Transclude_list_item_excerpts_as_random_slideshow
 13.60%  629.437      1 Template:Transclude_files_as_random_slideshow
  9.69%  448.723      1 Template:Transclude_selected_current_events
  1.38%   63.910      8 Template:Box-header_colour
  0.97%   45.055      1 Template:Machine_learning_bar
  0.95%   43.851      1 Template:Sidebar_with_collapsible_lists
  0.90%   41.472      1 Template:Portal_description
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:44942806-0!canonical!math=5 and timestamp 20190315120206 and revision id 887475509
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div> <div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509">https://en.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509</a>"					</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Computing_portals" title="Category:Computing portals">Computing portals</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Portals_with_short_description" title="Category:Portals with short description">Portals with short description</a></li><li><a href="/wiki/Category:Single-page_portals" title="Category:Single-page portals">Single-page portals</a></li><li><a href="/wiki/Category:All_portals" title="Category:All portals">All portals</a></li><li><a href="/wiki/Category:Portals_with_titles_not_starting_with_a_proper_noun" title="Category:Portals with titles not starting with a proper noun">Portals with titles not starting with a proper noun</a></li></ul></div></div> <div class="visualClear"></div>
</div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Portal%3AMachine+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Portal%3AMachine+learning" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li> </ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-portal"><span><a href="/wiki/Portal:Machine_learning">Portal</a></span></li><li id="ca-talk"><span><a accesskey="t" href="/wiki/Portal_talk:Machine_learning" rel="discussion" title="Discussion about the content page [t]">Talk</a></span></li> </ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><span><a href="/wiki/Portal:Machine_learning">Read</a></span></li><li class="collapsible" id="ca-edit"><span><a accesskey="e" href="/w/index.php?title=Portal:Machine_learning&amp;action=edit" title="Edit this page [e]">Edit</a></span></li><li class="collapsible" id="ca-history"><span><a accesskey="h" href="/w/index.php?title=Portal:Machine_learning&amp;action=history" title="Past revisions of this page [h]">View history</a></span></li> </ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label"><span>More</span></h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/><input name="title" type="hidden" value="Special:Search"/><input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/><input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/> </div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">Navigation</h3>
<div class="body">
<ul>
<li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">Interaction</h3>
<div class="body">
<ul>
<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">Tools</h3>
<div class="body">
<ul>
<li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Portal:Machine_learning" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Portal:Machine_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Portal:Machine_learning&amp;oldid=887475509" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Portal:Machine_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879" title="Link to connected data repository item [g]">Wikidata item</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">Print/export</h3>
<div class="body">
<ul>
<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Portal%3AMachine+learning">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Portal%3AMachine+learning&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Portal:Machine_learning&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">Languages</h3>
<div class="body">
<ul>
<li class="interlanguage-link interwiki-ar"><a class="interlanguage-link-target" href="https://ar.wikipedia.org/wiki/%D8%A8%D9%88%D8%A7%D8%A8%D8%A9:%D8%A7%D9%84%D8%AA%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A2%D9%84%D9%8A" hreflang="ar" lang="ar" title="بوابة:التعلم الآلي – Arabic">العربية</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Portail:Donn%C3%A9es/Machine_Learning" hreflang="fr" lang="fr" title="Portail:Données/Machine Learning – French">Français</a></li> </ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q58630879#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div> </div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 12 March 2019, at 22:21<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Portal:Machine_learning&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico">
<a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a> </li>
<li id="footer-poweredbyico">
<a href="//www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a> </li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"4.376","walltime":"4.880","ppvisitednodes":{"value":1163,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":70695,"limit":2097152},"templateargumentsize":{"value":1255,"limit":2097152},"expansiondepth":{"value":14,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":95382,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 4629.634      1 -total"," 96.51% 4467.870      1 Template:Flex_columns"," 41.68% 1929.819      1 Template:Transclude_selected_recent_additions"," 30.42% 1408.510      1 Template:Transclude_list_item_excerpts_as_random_slideshow"," 13.60%  629.437      1 Template:Transclude_files_as_random_slideshow","  9.69%  448.723      1 Template:Transclude_selected_current_events","  1.38%   63.910      8 Template:Box-header_colour","  0.97%   45.055      1 Template:Machine_learning_bar","  0.95%   43.851      1 Template:Sidebar_with_collapsible_lists","  0.90%   41.472      1 Template:Portal_description"]},"scribunto":{"limitreport-timeusage":{"value":"4.105","limit":"10.000"},"limitreport-memusage":{"value":5603815,"limit":52428800},"limitreport-logs":"table#1 {\n  [\"size\"] = \"tiny\",\n}\n"},"cachereport":{"origin":"mw1254","timestamp":"20190315120211","ttl":21600,"transientcontent":true}}});mw.config.set({"wgBackendResponseTime":4995,"wgHostname":"mw1254"});});</script>
</body>
</html>
